<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Machine Learning Learning Lab 4: Unsupervised Methods</title>
    <meta charset="utf-8" />
    <meta name="author" content="Dr. Joshua Rosenberg" />
    <meta name="date" content="2022-07-15" />
    <script src="libs/header-attrs-2.14/header-attrs.js"></script>
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/panelset-0.2.6/panelset.css" rel="stylesheet" />
    <script src="libs/panelset-0.2.6/panelset.js"></script>
    <link rel="stylesheet" href="css/laser.css" type="text/css" />
    <link rel="stylesheet" href="css/laser-fonts.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">


class: clear, title-slide, inverse, center, top, middle









# Machine Learning Learning Lab 4: Unsupervised Methods
----
### **Dr. Joshua Rosenberg**
### July 15, 2022

---

# Happy Friday

.panelset[

.panel[.panel-name[Recap]

- We began with _supervised machine learning_ and focused on its **predictive** emphasis
- We considered how to **engineer features**, using _k-folds cross-validation_ to make our training data go further
    - We did some serious debugging
- We discussed a more complex model - _a random forest_ - and how to engage in **model tuning** 
    - We haven't had time to work through the case study for this yet    
]

.panel[.panel-name[A few more thoughts]

- If you are still figuring out how you are planning to use ML . . . 
    - Great! Let's hear what you're thinking _at this time_
        - This is a lengthy process -- not something to finalize within one week
- Again, if you have a _specific use of ML in mind_ . . . 
    - Make sense of the labs
    - And, at the same time, begin to look for _how_ you can achieve your goal
        - R packages (recently updated, with good documentation, with active users, compatibility, papers)

]

.panel[.panel-name[Logistics]

- Please come see me at a break for your badge(s)!
    - We may need to get you your badge(s) at a later time 
- Links to readings are here: https://github.com/laser-institute/essential-readings/tree/main/machine-learning

]

.panel[.panel-name[Plan for today]

- Discussion and code-along for LL3
- Time to work on case study for LL3
- **BREAK!**
- Presentation for LL4
- Work time

]
]

---

# Discussion

.pull-left[

### Turn to an elbow partner and discuss:

- For what context/purpose might _discovering patterns or themes_ in data be helpful?
- Which data might you use for **unsupervised methods**?

]

.pull-right[

&lt;img src="img/IMG_8152.jpeg" width="100%" /&gt;

]

---

# Background

- Until now, we've used coded data to _train_ an algorithm
    - In short, we've used _supervised_ machine learning
    - But, we may not yet have codes; what options to do we have in such situations?
- We can turn to _unsupervised_ machine learning methods
- We'll use ASSISTments data to do so

---

# Agenda

.pull-left[

## Part 1: Core Concepts
### Latent Profile Analysis
- Latent Profile Analysis: Determining the number of profiles
- Latent Profile Analysis: Interpreting profiles
- Computational Grounded Theory
]

.pull-right[

## Part 2: R Code-Along
### tidyLPA 
- `tidyLPA()` functions
- ASSISTments data
]

---

class: clear, inverse, center, middle

# Core Concepts

---

# Unsupervised ML

- Does not require coded data; one way to think about unsupervised ML is that its purpose is to discover codes/labels
- Is used to discover groups among observations/cases or to summarize across variables
- Can be used in an _exploratory mode_ (see [Nelson, 2020](https://journals.sagepub.com/doi/full/10.1177/0049124118769114?casa_token=EV5XH31qbyAAAAAA%3AFg09JQ1XHOOzlxYT2SSJ06vZv0jG-s4Qfz8oDIQwh2jrZ-jrHNr7xZYL2FwnZtZiokhPalvV1RL2Bw)) 
- **Warning**: The results of unsupervised ML _cannot_ directly be used to provide codes/outcomes for supervised ML techniques 
- Can work with both continuous and dichotomous or categorical variables
- Algorithms include:
  - Latent Profile Analysis (and _general mixture models_
  - [Principle Components Analysis (really!)](https://web.stanford.edu/~hastie/ElemStatLearn/)
  - Latent Dirichlet Allocation (topic modeling)

---

# What technique should I choose?

Do you not yet have codes/outcomes -- and do you want to?

- _Achieve a starting point_ for qualitative coding, perhaps in a ["computational grounded theory"](https://journals.sagepub.com/doi/full/10.1177/0049124117729703) mode?
- _Discover groups or patterns in your data_ that may be of interest?
- _Reduce the number of variables in your dataset_ to a smaller, but perhaps nearly as explanatory/predictive - set of variables?

&lt;bold&gt;&lt;h4&gt;&lt;center&gt;Unsupervised methods may be helpful&lt;/center&gt;&lt;/h4&gt;&lt;/bold&gt;

---

# Range of data

- We can use unsupervised machine learning methods with a range of data types
    - Structured data:
        - Numeric data
        - Categorical data
    - Unstructured data:
        - Text
        - Images
        - Video

**We'll focus here on structured, numeric data** as a way into using these methods

---

# LPA

- Latent Profile Analysis can be considered to be an unsupervised machine learning method suited to the analysis of structured, numeric data
- It is closely related to more general _mixture_ models, namely Latent Class Analysis (for categorical data)
- Historically, it has been common for educational researchers (and psychologists) to estimate such models using proprietary software--MPlus
- But, a package in R is now available and widely-used, [tidyLPA](https://data-edu.github.io/tidyLPA/index.html)

*We'll use tidyLPA fr this learning lab*

---

# Computational Grounded Theory

- To draw a connection between LPA and machine learning, we'll consider its use in a part of a broader frame, _Computational Grounded Theory_
- Laura Nelson developed this approach in a pioneering paper ([Nelson, 2020](https://journals.sagepub.com/doi/full/10.1177/0049124117729703))
- It involves three steps:
    1. Unsupervised machine learning to _explore_ the data
    2. Careful qualitative analysis of the raw data _and_ the output from step 1
    3. Validation of the revised codes that result from steps 1 and 2
    
---

# Nelson's approach

![](https://journals.sagepub.com/na101/home/literatum/publisher/sage/journals/content/smra/2020/smra_49_1/0049124117729703/20200108/images/medium/10.1177_0049124117729703-fig2.gif)&lt;!-- --&gt;

---

# Using Computational Grounded Theory

- We can consider the results of LPA to not be a _final_, but an _initial_ step in the analysis
- After step 1 of computational grounded theory, the codes can be interrogated more deeply using _qualitative_ methods
- Then, the resulting codes can be validated:
    - Expert review
    - Referral to criterion/varied sources of validity evidence
    - Supervised machine learning methods

---

---

# An example in science education research

- In [Rosenberg and Krist (2021)](https://link.springer.com/article/10.1007/s10956-020-09862-4), students' written responses were the raw (unstructured) data that was used
- Though a coding frame existed, it was not well suited to the specific data that was available
- At the same time, there was _a lot_ of data available
- The three steps of computational grounded theory were carried out:
    1. Unsupervised exploration of the textual data (topic modeling)
    2. Careful qualitative analysis/reading of the textual data and the topics
    3. Validation (using supervised machine learning methods)

https://faast.shinyapps.io/generality-shiny/

---

# A bit more on using machine learning in science education

- Kubsch et al. (in press) ask how machine learning can be used in ways _beyond automation_ and the scaling of existing coding
- Unsupervised methods play a key role here
- *Distributing Epistemic Functions and Tasks (DEFT)* highlights the functions and tasks that pertain to generating knowledge that can be carried out by either skilled researchers or machine learning algorithms
    - Asking questions
    - Analyzing data
    - Interpreting findings

---

# Back to LPA

- There are several key steps in LPA:
    1. Choosing which variables to include
    1. Determining the number of profiles
    1. Interpreting the profile
    
**We'll explore each of these in the context of the PISA data next** -- then ASSISTments data in the case study

---

# ASSISTments

- We will use a portion of the ASSISTment data from a [data mining
competition](https://sites.google.com/view/assistmentsdatamining/home?authuser=0), identifying groups/patterns in learners' interactions with ASSISTments
- [This paper](https://educationaldatamining.org/EDM2014/uploads/procs2014/short%20papers/276_EDM-2014-Short.pdf) provides helpful context; sidebar: ~60% accuracy predicting student STEM enrollment (in college)
    - Features for student affect and behavior within ASSISTments, including: **boredom**, **engaged concentration**, **confusion**, and other (9 total)
- Developed from a classroom observational measure, BROMP, to identify these and _synchronized_ log files

---

# ASSISTments


```
## # A tibble: 514 × 9
##    ave_carelessness ave_know ave_correct ave_res_bored ave_res_engcon
##               &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;         &lt;dbl&gt;          &lt;dbl&gt;
##  1           0.159    0.255        0.380         0.223          0.650
##  2           0.0942   0.192        0.413         0.261          0.639
##  3           0.111    0.251        0.5           0.273          0.647
##  4           0.126    0.230        0.445         0.243          0.659
##  5           0.0932   0.146        0.310         0.211          0.656
##  6           0.0899   0.151        0.328         0.221          0.658
##  7           0.0691   0.117        0.291         0.199          0.665
##  8           0.0443   0.0826       0.204         0.198          0.687
##  9           0.0690   0.109        0.294         0.196          0.688
## 10           0.0749   0.133        0.317         0.230          0.670
## # … with 504 more rows, and 4 more variables: ave_res_conf &lt;dbl&gt;,
## #   ave_res_frust &lt;dbl&gt;, ave_res_offtask &lt;dbl&gt;, ave_res_gaming &lt;dbl&gt;
```

---

class: clear, inverse, center, middle

# Code Examples

---

# Estimating 3 groups

We can see all three steps (select variables, estimate profiles, interpret results) in this code chunk


```r
library(tidyLPA)
library(dplyr)

pisaUSA15 %&gt;%
  select(broad_interest, enjoyment, self_efficacy) %&gt;% # select three variables
  estimate_profiles(3) %&gt;%  # estimate 3 profiles
  plot_profiles(add_line = TRUE, rawdata = FALSE)
```

---

# Estimating 3 groups

![](ll-4-overview-presentation_files/figure-html/plot-3-1.png)&lt;!-- --&gt;

---

# Interpreting

- How might we interpret these profiles?
- What could we name them?
- How might we validate them?

---

# Comparing profiles
 

```r
pisaUSA15 %&gt;%
    select(broad_interest, enjoyment, self_efficacy) %&gt;%
    estimate_profiles(1:5) %&gt;% # note: we can also specify different model paramaterizations
    compare_solutions(statistics = c("AIC", "BIC"))
```

---

# Comparing profiles


```
## Warning in estimate_profiles_mclust(df_full, n_profiles, model_numbers, : The mclust algorithm does not allow for missing data. Some rows were omitted from analysis. Consider using Mplus, which accounts for cases with partially missing data, or use a non-parametric single imputation technique prior to analysis, such as the R-package 'missForest'.
```

```
## Warning: The solution with the maximum number of classes under consideration was
## considered to be the best solution according to one or more fit indices. Examine
## your results with care and consider estimating more classes.
```

```
## Compare tidyLPA solutions:
## 
##  Model Classes AIC       BIC      
##  1     1       35664.880 35704.418
##  1     2       33290.010 33355.906
##  1     3       32662.919 32755.172
##  1     4       30712.776 30831.387
##  1     5       30333.396 30478.365
## 
## Best model according to AIC is Model 1 with 5 classes.
## Best model according to BIC is Model 1 with 5 classes.
## 
## An analytic hierarchy process, based on the fit indices AIC, AWE, BIC, CLC, and KIC (Akogul &amp; Erisoglu, 2017), suggests the best solution is Model 1 with 5 classes.
```

---

# Saving the output of an analysis

Let's say we selected a model with _four_ profiles.


```
## Warning in estimate_profiles_mclust(df_full, n_profiles, model_numbers, : The mclust algorithm does not allow for missing data. Some rows were omitted from analysis. Consider using Mplus, which accounts for cases with partially missing data, or use a non-parametric single imputation technique prior to analysis, such as the R-package 'missForest'.
```

```
## tidyLPA analysis using mclust: 
## 
##  Model Classes AIC      BIC      Entropy prob_min prob_max n_min n_max BLRT_p
##  1     4       30712.73 30831.34 0.94    0.95     0.98     0.08  0.54  0.01
```
 
---

# Saving the output of an analysis


```
## Warning in estimate_profiles_mclust(df_full, n_profiles, model_numbers, : The mclust algorithm does not allow for missing data. Some rows were omitted from analysis. Consider using Mplus, which accounts for cases with partially missing data, or use a non-parametric single imputation technique prior to analysis, such as the R-package 'missForest'.
```

![](ll-4-overview-presentation_files/figure-html/unnamed-chunk-9-1.png)&lt;!-- --&gt;

---

# Other functions for working with LPA output


```r
get_estimates(m4)
get_data(m4)
get_fit(m4)
```

---

# In the remainder of this learning lab, you'll dive deeper into this model

- **Guided walkthrough**: We'll walk through a Latent Profile Analysis using data from the ASSISTments platform.
- **Case study**: Specify a latent profile analysis using your own data.

---

class: clear, center

## .font130[.center[**Thank you!**]]
&lt;br/&gt;
.center[&lt;img style="border-radius: 80%;" src="img/jr-cycling.jpeg" height="200px"/&gt;&lt;br/&gt;**Dr. Joshua Rosenberg**&lt;br/&gt;&lt;mailto:jmrosenberg@utk.edu&gt;]
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "default",
"highlightLines": true,
"highlightLanguage": "r",
"countIncrementalSlides": false,
"ratio": "16:9",
"slideNumberFormat": "<div class=\"progress-bar-container\">\n <div class=\"progress-bar\" style=\"width: calc(%current% / %total% * 100%);\">\n </div>\n</div>"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
