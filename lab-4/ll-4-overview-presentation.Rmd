---
title: "Machine Learning Learning Lab 4: Unsupervised Methods"
subtitle: "Overview Presentation"
author: "**Dr. Joshua Rosenberg**"
institute: "LASER Institute"
date: '`r format(Sys.time(), "%B %d, %Y")`'
output:
  xaringan::moon_reader:
    css:
     - default
     - css/laser.css
     - css/laser-fonts.css
    lib_dir: libs                        # creates directory for libraries
    seal: false                          # false: custom title slide
    nature:
      highlightStyle: default         # highlighting syntax for code
      highlightLines: true               # true: enables code line highlighting 
      highlightLanguage: ["r"]           # languages to highlight
      countIncrementalSlides: false      # false: disables counting of incremental slides
      ratio: "16:9"                      # 4:3 for standard size,16:9
      slideNumberFormat: |
       <div class="progress-bar-container">
        <div class="progress-bar" style="width: calc(%current% / %total% * 100%);">
        </div>
       </div>
---

class: clear, title-slide, inverse, center, top, middle

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r, echo=FALSE}
# then load all the relevant packages
pacman::p_load(pacman, knitr, tidyverse, readxl)
```

```{r xaringan-panelset, echo=FALSE}
xaringanExtra::use_panelset()
```

```{r xaringanExtra-clipboard, echo=FALSE}

```

# `r rmarkdown::metadata$title`
----
### `r rmarkdown::metadata$author`
### `r format(Sys.time(), "%B %d, %Y")`

---

# Happy Friday

.panelset[

.panel[.panel-name[Recap]

- We began with _supervised machine learning_ and focused on its **predictive** emphasis
- We considered how to **engineer features**, using _k-folds cross-validation_ to make our training data go further
    - We did some serious debugging
- We discussed a more complex model - _a random forest_ - and how to engage in **model tuning** 
    - We haven't had time to work through the case study for this yet    
]

.panel[.panel-name[A few more thoughts]

- If you are still figuring out how you are planning to use ML . . . 
    - Great! Let's hear what you're thinking _at this time_
        - This is a lengthy process -- not something to finalize within one week
- Again, if you have a _specific use of ML in mind_ . . . 
    - Make sense of the labs
    - And, at the same time, begin to look for _how_ you can achieve your goal
        - R packages (recently updated, with good documentation, with active users, compatibility, papers)

]

.panel[.panel-name[Logistics]

- Please come see me at a break for your badge(s)!
    - We may need to get you your badge(s) at a later time 
- Links to readings are here: https://github.com/laser-institute/essential-readings/tree/main/machine-learning

]

.panel[.panel-name[Plan for today]

- Discussion and code-along for LL3
- Time to work on case study for LL3
- **BREAK!**
- Presentation for LL4
- Work time

]
]

---

# Discussion

.pull-left[

### Turn to an elbow partner and discuss:

- For what context/purpose might _discovering patterns or themes_ in data be helpful?
- Which data might you use for **unsupervised methods**?

]

.pull-right[

```{r, echo = FALSE, out.width = "100%"}
knitr::include_graphics("img/IMG_8152.jpeg")
```

]

---

# Background

- Until now, we've used coded data to _train_ an algorithm
    - In short, we've used _supervised_ machine learning
    - But, we may not yet have codes; what options to do we have in such situations?
- We can turn to _unsupervised_ machine learning methods
- We'll use ASSISTments data to do so

---

# Agenda

.pull-left[

## Part 1: Core Concepts
### Latent Profile Analysis
- Latent Profile Analysis: Determining the number of profiles
- Latent Profile Analysis: Interpreting profiles
- Computational Grounded Theory
]

.pull-right[

## Part 2: R Code-Along
### tidyLPA 
- `tidyLPA()` functions
- ASSISTments data
]

---

class: clear, inverse, center, middle

# Core Concepts

---

# Unsupervised ML

- Does not require coded data; one way to think about unsupervised ML is that its purpose is to discover codes/labels
- Is used to discover groups among observations/cases or to summarize across variables
- Can be used in an _exploratory mode_ (see [Nelson, 2020](https://journals.sagepub.com/doi/full/10.1177/0049124118769114?casa_token=EV5XH31qbyAAAAAA%3AFg09JQ1XHOOzlxYT2SSJ06vZv0jG-s4Qfz8oDIQwh2jrZ-jrHNr7xZYL2FwnZtZiokhPalvV1RL2Bw)) 
- **Warning**: The results of unsupervised ML _cannot_ directly be used to provide codes/outcomes for supervised ML techniques 
- Can work with both continuous and dichotomous or categorical variables
- Algorithms include:
  - Latent Profile Analysis (and _general mixture models_
  - [Principle Components Analysis (really!)](https://web.stanford.edu/~hastie/ElemStatLearn/)
  - Latent Dirichlet Allocation (topic modeling)

---

# What technique should I choose?

Do you not yet have codes/outcomes -- and do you want to?

- _Achieve a starting point_ for qualitative coding, perhaps in a ["computational grounded theory"](https://journals.sagepub.com/doi/full/10.1177/0049124117729703) mode?
- _Discover groups or patterns in your data_ that may be of interest?
- _Reduce the number of variables in your dataset_ to a smaller, but perhaps nearly as explanatory/predictive - set of variables?

<bold><h4><center>Unsupervised methods may be helpful</center></h4></bold>

---

# Range of data

- We can use unsupervised machine learning methods with a range of data types
    - Structured data:
        - Numeric data
        - Categorical data
    - Unstructured data:
        - Text
        - Images
        - Video

**We'll focus here on structured, numeric data** as a way into using these methods

---

# LPA

- Latent Profile Analysis can be considered to be an unsupervised machine learning method suited to the analysis of structured, numeric data
- It is closely related to more general _mixture_ models, namely Latent Class Analysis (for categorical data)
- Historically, it has been common for educational researchers (and psychologists) to estimate such models using proprietary software--MPlus
- But, a package in R is now available and widely-used, [tidyLPA](https://data-edu.github.io/tidyLPA/index.html)

*We'll use tidyLPA fr this learning lab*

---

# Computational Grounded Theory

- To draw a connection between LPA and machine learning, we'll consider its use in a part of a broader frame, _Computational Grounded Theory_
- Laura Nelson developed this approach in a pioneering paper ([Nelson, 2020](https://journals.sagepub.com/doi/full/10.1177/0049124117729703))
- It involves three steps:
    1. Unsupervised machine learning to _explore_ the data
    2. Careful qualitative analysis of the raw data _and_ the output from step 1
    3. Validation of the revised codes that result from steps 1 and 2
    
---

# Nelson's approach

```{r, echo = FALSE}
knitr::include_graphics("https://journals.sagepub.com/na101/home/literatum/publisher/sage/journals/content/smra/2020/smra_49_1/0049124117729703/20200108/images/medium/10.1177_0049124117729703-fig2.gif")
```

---

# Using Computational Grounded Theory

- We can consider the results of LPA to not be a _final_, but an _initial_ step in the analysis
- After step 1 of computational grounded theory, the codes can be interrogated more deeply using _qualitative_ methods
- Then, the resulting codes can be validated:
    - Expert review
    - Referral to criterion/varied sources of validity evidence
    - Supervised machine learning methods

---

---

# An example in science education research

- In [Rosenberg and Krist (2021)](https://link.springer.com/article/10.1007/s10956-020-09862-4), students' written responses were the raw (unstructured) data that was used
- Though a coding frame existed, it was not well suited to the specific data that was available
- At the same time, there was _a lot_ of data available
- The three steps of computational grounded theory were carried out:
    1. Unsupervised exploration of the textual data (topic modeling)
    2. Careful qualitative analysis/reading of the textual data and the topics
    3. Validation (using supervised machine learning methods)

https://faast.shinyapps.io/generality-shiny/

---

# A bit more on using machine learning in science education

- Kubsch et al. (in press) ask how machine learning can be used in ways _beyond automation_ and the scaling of existing coding
- Unsupervised methods play a key role here
- *Distributing Epistemic Functions and Tasks (DEFT)* highlights the functions and tasks that pertain to generating knowledge that can be carried out by either skilled researchers or machine learning algorithms
    - Asking questions
    - Analyzing data
    - Interpreting findings

---

# Back to LPA

- There are several key steps in LPA:
    1. Choosing which variables to include
    1. Determining the number of profiles
    1. Interpreting the profile
    
**We'll explore each of these in the context of the PISA data next** -- then ASSISTments data in the case study

---

# ASSISTments

- We will use a portion of the ASSISTment data from a [data mining
competition](https://sites.google.com/view/assistmentsdatamining/home?authuser=0), identifying groups/patterns in learners' interactions with ASSISTments
- [This paper](https://educationaldatamining.org/EDM2014/uploads/procs2014/short%20papers/276_EDM-2014-Short.pdf) provides helpful context; sidebar: ~60% accuracy predicting student STEM enrollment (in college)
    - Features for student affect and behavior within ASSISTments, including: **boredom**, **engaged concentration**, **confusion**, and other (9 total)
- Developed from a classroom observational measure, BROMP, to identify these and _synchronized_ log files

---

# ASSISTments

```{r, echo = FALSE, message = FALSE}
library(readr)
library(dplyr)

d <- read_csv("data/dat_csv_combine_final_full.csv") %>% 
      select(AveCarelessness, AveKnow, AveCorrect = AveCorrect.x, AveResBored, 
         AveResEngcon, AveResConf, AveResFrust, AveResOfftask, 
         AveResGaming) %>%
    janitor::clean_names()

d
```

---

class: clear, inverse, center, middle

# Code Examples

---

# Estimating 3 groups

We can see all three steps (select variables, estimate profiles, interpret results) in this code chunk

```{r, eval = FALSE, echo = TRUE}
library(tidyLPA)
library(dplyr)

pisaUSA15 %>%
  select(broad_interest, enjoyment, self_efficacy) %>% # select three variables
  estimate_profiles(3) %>%  # estimate 3 profiles
  plot_profiles(add_line = TRUE, rawdata = FALSE)
```

---

# Estimating 3 groups

```{r plot-3, echo = FALSE, message = FALSE, warning = FALSE}
library(tidyLPA)
library(dplyr)

pisaUSA15 %>%
  select(broad_interest, enjoyment, self_efficacy) %>% # select three variables
  estimate_profiles(3) %>%  # estimate 3 profiles
  plot_profiles(add_line = TRUE, rawdata = FALSE)
```

---

# Interpreting

- How might we interpret these profiles?
- What could we name them?
- How might we validate them?

---

# Comparing profiles
 
```{r, echo = TRUE, eval = FALSE}
pisaUSA15 %>%
    select(broad_interest, enjoyment, self_efficacy) %>%
    estimate_profiles(1:5) %>% # note: we can also specify different model paramaterizations
    compare_solutions(statistics = c("AIC", "BIC"))
```

---

# Comparing profiles

```{r, echo = FALSE, eval = TRUE}
pisaUSA15 %>%
    select(broad_interest, enjoyment, self_efficacy) %>%
    estimate_profiles(1:5) %>% # note: we can also specify different model paramaterizations
    compare_solutions(statistics = c("AIC", "BIC"))
```

---

# Saving the output of an analysis

Let's say we selected a model with _four_ profiles.

```{r}
m4 <- pisaUSA15 %>%
  select(broad_interest, enjoyment, self_efficacy) %>% # select three variables
  estimate_profiles(4)  # estimate 4 profiles

m4
```
 
---

# Saving the output of an analysis

```{r}
m4 <- pisaUSA15 %>%
  select(broad_interest, enjoyment, self_efficacy) %>% # select three variables
  estimate_profiles(4)  # estimate 4 profiles

m4 %>% 
    plot_profiles(add_line = TRUE, rawdata = FALSE)
```

---

# Other functions for working with LPA output

```{r, eval = FALSE, echo = TRUE}
get_estimates(m4)
get_data(m4)
get_fit(m4)
```

---

# In the remainder of this learning lab, you'll dive deeper into this model

- **Guided walkthrough**: We'll walk through a Latent Profile Analysis using data from the ASSISTments platform.
- **Case study**: Specify a latent profile analysis using your own data.

---

class: clear, center

## .font130[.center[**Thank you!**]]
<br/>
.center[<img style="border-radius: 80%;" src="img/jr-cycling.jpeg" height="200px"/><br/>**Dr. Joshua Rosenberg**<br/><mailto:jmrosenberg@utk.edu>]
